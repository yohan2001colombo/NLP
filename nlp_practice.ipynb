{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "859e7da0-0afa-49da-b2ac-0e7863018a6b",
   "metadata": {},
   "source": [
    "# **Tutorial 01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e19214-6b75-455e-83a5-8c40fdeef510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd1790e3-6487-4328-9ffa-026c9b319377",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, \n",
    "and nothing particular to interest me on shore, I thought I would sail about a little and see \n",
    "the watery part of the world.  \n",
    "It is a way I have of driving off the spleen and regulating the circulation. \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9729718-508e-4b01-8952-f24ddf382fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tokens=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c702e3-02ab-418b-87bb-c85f7cb41635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call',\n",
       " 'me',\n",
       " 'Ishmael',\n",
       " '.',\n",
       " 'Some',\n",
       " 'years',\n",
       " 'ago—never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long',\n",
       " 'precisely—having',\n",
       " 'little',\n",
       " 'or',\n",
       " 'no',\n",
       " 'money',\n",
       " 'in',\n",
       " 'my',\n",
       " 'purse',\n",
       " ',',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'to',\n",
       " 'interest',\n",
       " 'me',\n",
       " 'on',\n",
       " 'shore',\n",
       " ',',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'I',\n",
       " 'would',\n",
       " 'sail',\n",
       " 'about',\n",
       " 'a',\n",
       " 'little',\n",
       " 'and',\n",
       " 'see',\n",
       " 'the',\n",
       " 'watery',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'a',\n",
       " 'way',\n",
       " 'I',\n",
       " 'have',\n",
       " 'of',\n",
       " 'driving',\n",
       " 'off',\n",
       " 'the',\n",
       " 'spleen',\n",
       " 'and',\n",
       " 'regulating',\n",
       " 'the',\n",
       " 'circulation',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcc245e-e191-4688-8f93-8153fc041954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert nltk text object\n",
    "obj=Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f69d4e3-3e45-4974-ad75-250e9efd7b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Call me Ishmael . Some years ago—never mind...>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0912ded5-f7d7-4f72-b88b-36eaaefdaf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1395cf74-1bcb-4588-8c8c-3d5de553d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "615f37cf-76ae-465e-9b50-ddf75ecfa4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6381097-0d9b-4e66-a11d-c6e48049af99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 11 of 11 matches:\n",
      "ong the former , one was of a most monstrous size . ... This came towards us , \n",
      "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
      "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
      "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
      "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
      "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
      "th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n",
      "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
      "ere to enter upon those still more monstrous stories of them which are to be fo\n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
      "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n"
     ]
    }
   ],
   "source": [
    "text1.concordance('monstrous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "448460c8-3fcb-4a94-ad24-ccff5ba7ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "rest me on shore , I thought I would sail about a little and see the watery pa\n"
     ]
    }
   ],
   "source": [
    "obj.concordance('sail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f29f565-b60b-4e9b-b336-380e9966926a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.count(\"whale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88bc2cc7-4827-4b60-bb56-a9c40ef909c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.count(\"whale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b85683b-e43f-4dda-a753-e6564bb2167e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bfd52db-a6fc-41c8-871b-c920a276bff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b97c7e-38b9-4eb9-8c40-99fd88aa89b7",
   "metadata": {},
   "source": [
    "# **Tutorial 02**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a79a54b-8988-472f-bc93-7924809f76d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true contemptible christian abundant few part mean careful puzzled\n",
      "mystifying passing curious loving wise doleful gamesome singular\n",
      "delightfully perilous fearless\n"
     ]
    }
   ],
   "source": [
    "text1.similar('monstrous')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2bc04-d9d0-4d85-a655-a96ef5c93800",
   "metadata": {},
   "source": [
    "# **Tutorial 03**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "751fc3aa-6f49-42e9-8d64-2e1eefcdac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronouncing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af348553-eddb-4551-b13a-5caa44c2c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"House\",\"mother\",\"father\",\"Sadini\"]\n",
    "\n",
    "tr = {}\n",
    "for i in words:\n",
    "    phones = pronouncing.phones_for_word(i)\n",
    "    if phones:  # list is not empty\n",
    "        tr[i] = phones[0]\n",
    "    else:\n",
    "        tr[i] = \"N/A\"  # or handle as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4690315b-c2f0-46cd-aac3-15da063df5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'House': 'HH AW1 S',\n",
       " 'mother': 'M AH1 DH ER0',\n",
       " 'father': 'F AA1 DH ER0',\n",
       " 'Sadini': 'N/A'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4e8b251-3778-4ec4-901b-431b71b5ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average syllables per word: 2.2\n"
     ]
    }
   ],
   "source": [
    "import pyphen\n",
    "\n",
    "# 1. Create a Pyphen dictionary for English\n",
    "dic = pyphen.Pyphen(lang='en')\n",
    "\n",
    "# 2. Example list of words (you can replace this with your own list)\n",
    "filtered = ['beautiful', 'syllable', 'language', 'poetry', 'structure']\n",
    "\n",
    "# 3. Count syllables for each word\n",
    "syllables = [len(dic.inserted(w).split('-')) for w in filtered]\n",
    "\n",
    "# 4. Calculate and print average syllables per word\n",
    "average = sum(syllables) / len(syllables)\n",
    "print(\"Average syllables per word:\", average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edff00e4-3231-4a43-8604-530b79a5ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "dic=pyphen.Pyphen(lang='en')\n",
    "syllables = [dic.inserted(i).split('-') for i in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "998ae127-f0a7-4657-907f-3ee617507c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(dic.inserted(i).split('-')) for i in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23777760-9abf-47b0-9fcb-493db476f37d",
   "metadata": {},
   "source": [
    "# **Tutorial 03**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5a696b7-4b16-453c-88dc-77d779d3d031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')  #punkt is a pre-trained sentence tokenizer model\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04082631-0bc1-4ca1-aa68-9ccf67b3c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e970a95-b58e-4988-a698-0c056bc80d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "China's economy grew by 3% last year, far below the government's target. The country faced \n",
    "multiple challenges, including strict Covid-19 measures, a property market slump, and \n",
    "weak consumer demand. \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cb79380-f8bb-4498-9664-f5a6b2d156c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nChina's economy grew by 3% last year, far below the government's target. The country faced \\nmultiple challenges, including strict Covid-19 measures, a property market slump, and \\nweak consumer demand. \\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26262eee-825b-4526-84b7-8290673f00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.sub(r'[^a-zA-Z0-9\\s]','',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "639fcfd8-5685-42db-8c28-08ef2a7f472f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nChinas economy grew by 3 last year far below the governments target The country faced \\nmultiple challenges including strict Covid19 measures a property market slump and \\nweak consumer demand \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1985d3ae-5d9b-4f56-97a7-26fd5bcf243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "103111b1-ffc6-4ea3-aec8-484d3ddf2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8595ee13-8f43-467b-8798-093cb002a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens= len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fdb6e08-5371-4166-b1ab-7f81037bb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens=len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b071270-d7cc-4be2-9d85-7ac1f2c9bc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9655172413793104"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens/total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14ac3c82-ebb4-4f14-8b25-37ef4aba018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7143e73b-8968-4231-b815-29f9d49528c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb8644ac-f6fd-49d2-8dab-37a744539c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_stop=[w for w in tokens if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97d02c0d-12f0-4728-a3b5-5c7fc9b440da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ea00e37-60a6-4083-bb35-446aea79b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4daceeca-dd7f-4db6-9ffe-8444205b7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_stemmed=[stemmer.stem(i) for i in tokens_without_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2163278-7877-41b8-837d-04ccd725fc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china',\n",
       " 'economi',\n",
       " 'grew',\n",
       " '3',\n",
       " 'last',\n",
       " 'year',\n",
       " 'far',\n",
       " 'govern',\n",
       " 'target',\n",
       " 'countri',\n",
       " 'face',\n",
       " 'multipl',\n",
       " 'challeng',\n",
       " 'includ',\n",
       " 'strict',\n",
       " 'covid19',\n",
       " 'measur',\n",
       " 'properti',\n",
       " 'market',\n",
       " 'slump',\n",
       " 'weak',\n",
       " 'consum',\n",
       " 'demand']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d37970df-98ca-4cea-a26c-1b22e6414fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ad09bcd-2355-467f-a228-18dfe857d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a34396d-84c0-4892-b5c9-8fb8250955d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d002a6da-051e-4094-9a2e-d701a13edce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chinas',\n",
       " 'economy',\n",
       " 'grew',\n",
       " 'by',\n",
       " '3',\n",
       " 'last',\n",
       " 'year',\n",
       " 'far',\n",
       " 'below',\n",
       " 'the',\n",
       " 'governments',\n",
       " 'target',\n",
       " 'The',\n",
       " 'country',\n",
       " 'faced',\n",
       " 'multiple',\n",
       " 'challenges',\n",
       " 'including',\n",
       " 'strict',\n",
       " 'Covid19',\n",
       " 'measures',\n",
       " 'a',\n",
       " 'property',\n",
       " 'market',\n",
       " 'slump',\n",
       " 'and',\n",
       " 'weak',\n",
       " 'consumer',\n",
       " 'demand']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab3a3638-4ebc-4b83-a259-5ec5f16aacd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## chatgpt example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "efdd94ac-14b9-4cbb-87d2-471e4bd82af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.', 'Smith', 'is', \"n't\", 'going', 'to', 'the', 'U.S.', 'today', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')  # Still required here too!\n",
    "\n",
    "text = \"Dr. Smith isn't going to the U.S. today.\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "561a6609-7870-401a-b2c0-5de6767e5840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Smith is here.', 'He arrived at 9 a.m.', \"Isn't that early?\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')  # Downloads the punkt model\n",
    "\n",
    "text = \"Dr. Smith is here. He arrived at 9 a.m. Isn't that early?\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67281ece-8719-4dac-89e2-3b7e164d9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Handles abbreviations (Dr., U.S.), decimals (3.14), and other edge cases without splitting incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803c569-0602-4b7c-af10-b92113ca34f2",
   "metadata": {},
   "source": [
    "# **Tutorial 06**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095863e-72b6-4884-bf3a-59ed6314cfe7",
   "metadata": {},
   "source": [
    "#### ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a84f4dc-511b-42b4-ad2b-fcbf1000a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "698a7af7-5c81-4b31-937d-1dcef14a4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a function for ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d5a50c9-f3cc-4229-8ef1-c9e99c711391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Download tokenizer models (required for word_tokenize)\n",
    "\n",
    "def extract_ngrams(data, num):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text and returns a list of n-grams (as strings).\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(data)\n",
    "    n_grams = ngrams(tokens, num)\n",
    "    return [' '.join(grams) for grams in n_grams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2291c2c-d855-44e9-9eb6-996295014c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [\"China 's\", \"'s economy\", 'economy grew', 'grew by', 'by 3', '3 %', '% last', 'last year', 'year ,', ', far']\n",
      "Trigrams: [\"China 's economy\", \"'s economy grew\", 'economy grew by', 'grew by 3', 'by 3 %', '3 % last', '% last year', 'last year ,', 'year , far', ', far below']\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "My_text = \"\"\"China's economy grew by 3% last year, far below the government's target.\n",
    "The country faced multiple challenges, including strict Covid-19 measures, a property market slump, and weak consumer demand.\"\"\"\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams = extract_ngrams(My_text, 2)\n",
    "print(\"Bigrams:\", bigrams[:10])  # Show first 10 bigrams\n",
    "\n",
    "# Generate trigrams\n",
    "trigrams = extract_ngrams(My_text, 3)\n",
    "print(\"Trigrams:\", trigrams[:10])  # Show first 10 trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2526093-59de-47aa-8b4c-69c5c9ded0fd",
   "metadata": {},
   "source": [
    "### Explanation of `return [' '.join(grams) for grams in n_grams]`\n",
    "\n",
    "This line is a **list comprehension** that converts each n-gram tuple into a readable string format by joining the words with spaces.\n",
    "\n",
    "- `n_grams` is a list of tuples, where each tuple contains `n` consecutive words from the text.\n",
    "- `for grams in n_grams` iterates through each n-gram tuple.\n",
    "- `' '.join(grams)` joins the words in each tuple into a single string, separated by a space.\n",
    "\n",
    "#### Example:\n",
    "If `n_grams = [('China', \"'s\"), (\"'s\", 'economy')]`,  \n",
    "then this line returns:\n",
    "\n",
    "```python\n",
    "[\"China 's\", \"'s economy\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876237f5-083b-44cb-933f-e84c8e94a792",
   "metadata": {},
   "source": [
    "# **Tutorial 07**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd3b36-e41d-4f4f-bf59-2bfe5165e452",
   "metadata": {},
   "source": [
    "### 📚 NLTK's Treebank Corpus\n",
    "\n",
    "- The **Treebank corpus** in NLTK is a collection of English sentences with **syntactic annotations**.\n",
    "- It is based on the **Penn Treebank**, a widely used dataset in computational linguistics.\n",
    "\n",
    "#### Key features:\n",
    "- **POS-tagged sentences:** Each word is labeled with its Part-of-Speech tag (e.g., noun, verb).\n",
    "- **Parsed sentences:** Sentences are annotated with **full syntactic parse trees** showing phrase structure.\n",
    "- **Human-annotated:** Considered gold-standard data for training and evaluating parsers.\n",
    "\n",
    "#### Usage in NLTK:\n",
    "- Access tagged sentences: lists of `(word, POS)` pairs.\n",
    "- Access parsed sentences: hierarchical tree structures representing grammar.\n",
    "\n",
    "#### Example code snippet:\n",
    "```python\n",
    "from nltk.corpus import treebank\n",
    "tagged_sents = treebank.tagged_sents()\n",
    "parsed_sents = treebank.parsed_sents()\n",
    "\n",
    "print(tagged_sents[0])        # Tagged sentence example\n",
    "parsed_sents[0].pretty_print() # Display parse tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58352349-408b-4a6b-81b5-3ad9490d3daf",
   "metadata": {},
   "source": [
    "## 🌳 NLTK's Treebank Corpus\n",
    "\n",
    "The **Treebank corpus** in NLTK is a part of the **Penn Treebank** — a famous linguistic dataset that includes syntactically analyzed English sentences.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Key Features\n",
    "\n",
    "#### 1. POS-tagged sentences**  \n",
    "  Each word is labeled with its **Part-of-Speech (POS)** tag, such as:\n",
    "  - `'NN'`: Noun  \n",
    "  - `'VB'`: Verb  \n",
    "  - `'JJ'`: Adjective  \n",
    "\n",
    "  **Example:**  \n",
    "  ```python\n",
    "  [('The', 'DT'), ('dog', 'NN'), ('barked', 'VBD')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2d5ae-cda7-42da-895c-4b68a8240a0d",
   "metadata": {},
   "source": [
    "### 2. Parsed Sentences (Syntax Trees)\n",
    "\n",
    "- Sentences in the Treebank corpus are represented as **hierarchical parse trees**.\n",
    "- These trees show how words are grouped into **phrases** like:\n",
    "  - **NP**: Noun Phrase\n",
    "  - **VP**: Verb Phrase\n",
    "- It helps in analyzing and understanding the **grammatical structure** of a sentence.\n",
    "\n",
    "#### 🧾 Example Parse Tree:\n",
    "\n",
    "(S\n",
    "  (NP (DT The) (NN dog))\n",
    "  (VP (VBD barked)))\n",
    "\n",
    "\n",
    "Explanation:\n",
    "- `S`: the whole sentence\n",
    "- `NP`: \"The dog\" is a noun phrase\n",
    "- `VP`: \"barked\" is a verb phrase\n",
    "- `DT`: Determiner (\"The\")\n",
    "- `NN`: Noun (\"dog\")\n",
    "- `VBD`: Past-tense verb (\"barked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b860c426-c479-4c12-97ab-4f5d276f7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "## chatgpt example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f259eb56-9e53-4f76-8b20-3f3fb7d0accb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\sadin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n",
      "                                                     S                                                                         \n",
      "                         ____________________________|_______________________________________________________________________   \n",
      "                        |                                               VP                                                   | \n",
      "                        |                        _______________________|___                                                 |  \n",
      "                      NP-SBJ                    |                           VP                                               | \n",
      "         _______________|___________________    |     ______________________|______________________________________          |  \n",
      "        |          |              ADJP      |   |    |        |                PP-CLR                              |         | \n",
      "        |          |           ____|____    |   |    |        |          ________|_________                        |         |  \n",
      "        NP         |          NP        |   |   |    |        NP        |                  NP                    NP-TMP      | \n",
      "   _____|____      |     _____|____     |   |   |    |     ___|____     |    ______________|__________        _____|_____    |  \n",
      " NNP        NNP    ,    CD        NNS   JJ  ,   MD   VB   DT       NN   IN  DT             JJ         NN    NNP          CD  . \n",
      "  |          |     |    |          |    |   |   |    |    |        |    |   |              |          |      |           |   |  \n",
      "Pierre     Vinken  ,    61       years old  ,  will join the     board  as  a         nonexecutive director Nov.         29  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "# Download if necessary\n",
    "nltk.download('treebank')\n",
    "\n",
    "# Get tagged sentences (words + POS tags)\n",
    "tagged_sents = treebank.tagged_sents()\n",
    "print(tagged_sents[0])\n",
    "\n",
    "# Get parsed sentences as trees\n",
    "parsed_sents = treebank.parsed_sents()\n",
    "print(parsed_sents[0])\n",
    "parsed_sents[0].pretty_print()  # Visualize the parse tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8bf90ff-a849-4172-9157-8a4dfdce53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### tutorial 07 cts.........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8f40f01c-bb72-4bd9-87cd-ebef39e5e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "data = treebank.tagged_sents() \n",
    "train_data = data[:3500] \n",
    "test_data = data[3500:] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5057de-e110-4563-b414-68c446ec2870",
   "metadata": {},
   "source": [
    "## 📑 Complete Penn Treebank POS Tags (Used in NLTK's Treebank Corpus)\n",
    "\n",
    "These tags are used to annotate each word in the corpus with its grammatical role.\n",
    "\n",
    "| Tag   | Description                               | Example(s)               |\n",
    "|--------|-------------------------------------------|---------------------------|\n",
    "| CC     | Coordinating conjunction                 | and, but, or              |\n",
    "| CD     | Cardinal number                          | one, two, 100             |\n",
    "| DT     | Determiner                               | the, a, an, this          |\n",
    "| EX     | Existential there                        | there (is)                |\n",
    "| FW     | Foreign word                             | d'hoevre, inter alia      |\n",
    "| IN     | Preposition or subordinating conjunction | in, of, like, although    |\n",
    "| JJ     | Adjective                                | green, quick              |\n",
    "| JJR    | Adjective, comparative                   | greener, faster           |\n",
    "| JJS    | Adjective, superlative                   | greenest, fastest         |\n",
    "| LS     | List item marker                         | 1., A., a)                |\n",
    "| MD     | Modal                                     | can, should, will         |\n",
    "| NN     | Noun, singular or mass                   | dog, money                |\n",
    "| NNS    | Noun, plural                             | dogs, houses              |\n",
    "| NNP    | Proper noun, singular                    | John, London              |\n",
    "| NNPS   | Proper noun, plural                      | Americans, Beatles        |\n",
    "| PDT    | Predeterminer                            | all, both, half           |\n",
    "| POS    | Possessive ending                        | ’s, ’                     |\n",
    "| PRP    | Personal pronoun   \n",
    "| I, he, you, me            |\n",
    "| RB     | Adverb                                   | quickly, very             |\n",
    "| RBR    | Adverb, comparative                      | faster, better            |\n",
    "| RBS    | Adverb, superlative                      | fastest, best             |\n",
    "| RP     | Particle                                  | up, off, out              |\n",
    "| TO     | to                                       | to (as in to go)          |\n",
    "| UH     | Interjection                             | uh, oh, oops              |\n",
    "| VB     | Verb, base form                          | run, eat                  |\n",
    "| VBD    | Verb, past tense                         | ran, ate                  |\n",
    "| VBG    | Verb, gerund/present participle          | running, eating           |\n",
    "| VBN    | Verb, past participle                   | run, eaten                |\n",
    "| VBP    | Verb, non-3rd person singular present    | run, eat                  |\n",
    "| VBZ    | Verb, 3rd person singular present        | runs, eats                |\n",
    "| WDT    | Wh-determiner                           | which, that               |\n",
    "| WP     | Wh-pronoun                              | who, what                 |\n",
    "| WP$    | Possessive wh-pronoun                   | whose                     |\n",
    "| WRB    | Wh-adverb                               | where, when, how          |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec680f-45a1-40ac-a3d9-53ed548c4cff",
   "metadata": {},
   "source": [
    "### 🧠 DefaultTagger in NLTK\n",
    "\n",
    "- The `DefaultTagger` is a simple POS tagger that assigns the **same tag** (e.g., `'NN'` for nouns) to every word.\n",
    "- It is used as a **baseline** to compare the performance of more advanced models.\n",
    "\n",
    "#### ✅ Code Explanation:\n",
    "```python\n",
    "from nltk.tag import DefaultTagger\n",
    "\n",
    "dt = DefaultTagger('NN')               # Tags everything as 'NN' (noun)\n",
    "accuracy = dt.evaluate(test_data)      # Evaluates against tagged test data\n",
    "tagged = dt.tag(tokens)                # Tags new tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83ecfecd-711d-4cc0-8831-5b9297418265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadin\\AppData\\Local\\Temp\\ipykernel_9764\\1004978388.py:3: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  default_tagger_accuracy = dt.evaluate(test_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default tagger accuracy: 0.15\n",
      "Default tagger example: [('Chinas', 'NN'), ('economy', 'NN'), ('grew', 'NN'), ('by', 'NN'), ('3', 'NN'), ('last', 'NN'), ('year', 'NN'), ('far', 'NN'), ('below', 'NN'), ('the', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger \n",
    "dt = DefaultTagger('NN') \n",
    "default_tagger_accuracy = dt.evaluate(test_data) \n",
    "default_tagger_example = dt.tag(tokens) \n",
    "print(f'Default tagger accuracy: {default_tagger_accuracy:.2f}') \n",
    "print(f'Default tagger example: {default_tagger_example[:10]}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95169846-38e6-47f6-9b3e-6885afdcb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import DefaultTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3316d6ff-81c8-41d0-8ab2-8fabf8e99b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DefaultTagger('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c6cc2e0-e5f4-4a28-ae92-94313b2ff1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadin\\AppData\\Local\\Temp\\ipykernel_9764\\2353188706.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  dt.evaluate(test_data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1454158195372253"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "539b8df9-7106-4ded-86ef-23a6fc8bf161",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dt.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f6afc0f7-fdad-43b0-870b-1b71f85aab64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chinas', 'NN'),\n",
       " ('economy', 'NN'),\n",
       " ('grew', 'NN'),\n",
       " ('by', 'NN'),\n",
       " ('3', 'NN'),\n",
       " ('last', 'NN'),\n",
       " ('year', 'NN'),\n",
       " ('far', 'NN'),\n",
       " ('below', 'NN'),\n",
       " ('the', 'NN'),\n",
       " ('governments', 'NN'),\n",
       " ('target', 'NN'),\n",
       " ('The', 'NN'),\n",
       " ('country', 'NN'),\n",
       " ('faced', 'NN'),\n",
       " ('multiple', 'NN'),\n",
       " ('challenges', 'NN'),\n",
       " ('including', 'NN'),\n",
       " ('strict', 'NN'),\n",
       " ('Covid19', 'NN'),\n",
       " ('measures', 'NN'),\n",
       " ('a', 'NN'),\n",
       " ('property', 'NN'),\n",
       " ('market', 'NN'),\n",
       " ('slump', 'NN'),\n",
       " ('and', 'NN'),\n",
       " ('weak', 'NN'),\n",
       " ('consumer', 'NN'),\n",
       " ('demand', 'NN')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae4f29-fd66-4588-bbb8-c7de8f9a0dd0",
   "metadata": {},
   "source": [
    "### Regex Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7578bfa-1322-4b76-8e54-e30c06063f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import RegexpTagger \n",
    "patterns = [ \n",
    "    (r'.*ing$', 'VBG'),       # e.g., 'running', 'jumping'\n",
    "    (r'.*ed$', 'VBD'),        # e.g., 'played', 'walked'\n",
    "    (r'.*es$', 'VBZ'),        # e.g., 'goes', 'watches'\n",
    "    (r'.ould$', 'MD'),        # e.g., 'could', 'should', 'would'\n",
    "    (r'.*\\'s$', 'NN$'),       # e.g., \"John's\", \"teacher's\"\n",
    "    (r'.*s$', 'NNS'),         # e.g., 'dogs', 'cars' (plural nouns)\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # e.g., '123', '-99.5' (cardinal numbers)\n",
    "    (r'.*', 'NN')             # Default case: noun\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "84062a3a-03ed-4026-90fa-db63894f1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt=RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d8aa1dfd-20f5-4153-83a0-a9176f8976d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadin\\AppData\\Local\\Temp\\ipykernel_9764\\787372689.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  regex_tagger_accuracy = rt.evaluate(test_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex tagger accuracy: 0.24\n",
      "Regex tagger example: [('Chinas', 'NNS'), ('economy', 'NN'), ('grew', 'NN'), ('by', 'NN'), ('3', 'CD'), ('last', 'NN'), ('year', 'NN'), ('far', 'NN'), ('below', 'NN'), ('the', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "regex_tagger_accuracy = rt.evaluate(test_data) \n",
    "regex_tagger_example = rt.tag(tokens) \n",
    "print(f'Regex tagger accuracy: {regex_tagger_accuracy:.2f}') \n",
    "print(f'Regex tagger example: {regex_tagger_example[:10]}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7cf29b27-255b-431c-b205-007fc10ee9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chinas',\n",
       " 'economy',\n",
       " 'grew',\n",
       " 'by',\n",
       " '3',\n",
       " 'last',\n",
       " 'year',\n",
       " 'far',\n",
       " 'below',\n",
       " 'the',\n",
       " 'governments',\n",
       " 'target',\n",
       " 'The',\n",
       " 'country',\n",
       " 'faced',\n",
       " 'multiple',\n",
       " 'challenges',\n",
       " 'including',\n",
       " 'strict',\n",
       " 'Covid19',\n",
       " 'measures',\n",
       " 'a',\n",
       " 'property',\n",
       " 'market',\n",
       " 'slump',\n",
       " 'and',\n",
       " 'weak',\n",
       " 'consumer',\n",
       " 'demand']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "659f192e-ad9b-41a0-b5d1-cb678a3157b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('About', 'IN'), ('30', 'CD'), ('%', 'NN'), ('of', 'IN'), ('Ratners', 'NNP'), (\"'s\", 'POS'), ('profit', 'NN'), ('already', 'RB'), ('is', 'VBZ'), ('derived', 'VBN'), ('*-1', '-NONE-'), ('from', 'IN'), ('the', 'DT'), ('U.S.', 'NNP'), ('.', '.')], [('Carnival', 'NNP'), ('Cruise', 'NNP'), ('Lines', 'NNP'), ('Inc.', 'NNP'), ('said', 'VBD'), ('0', '-NONE-'), ('potential', 'JJ'), ('problems', 'NNS'), ('with', 'IN'), ('the', 'DT'), ('construction', 'NN'), ('of', 'IN'), ('two', 'CD'), ('big', 'JJ'), ('cruise', 'NN'), ('ships', 'NNS'), ('from', 'IN'), ('Finland', 'NNP'), ('have', 'VBP'), ('been', 'VBN'), ('averted', 'VBN'), ('*-1', '-NONE-'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b4bb9-f31b-4210-ae34-24eaeec0e6dd",
   "metadata": {},
   "source": [
    "### Ngram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a998426b-7cef-49c2-8b39-a067c37d246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadin\\AppData\\Local\\Temp\\ipykernel_9764\\4252099450.py:5: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  unigram_tagger_accuracy = ut.evaluate(test_data)\n",
      "C:\\Users\\sadin\\AppData\\Local\\Temp\\ipykernel_9764\\4252099450.py:6: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  bigram_tagger_accuracy = bt.evaluate(test_data)\n",
      "C:\\Users\\sadin\\AppData\\Local\\Temp\\ipykernel_9764\\4252099450.py:7: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  trigram_tagger_accuracy = tt.evaluate(test_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram tagger accuracy: 0.86\n",
      "Bigram tagger accuracy: 0.13\n",
      "Trigram tagger accuracy: 0.08\n",
      "Unigram tagger example: [('Chinas', None), ('economy', 'NN'), ('grew', 'VBD'), ('by', 'IN'), ('3', 'CD'), ('last', 'JJ'), ('year', 'NN'), ('far', 'RB'), ('below', 'IN'), ('the', 'DT')]\n",
      "Bigram tagger example: [('Chinas', None), ('economy', None), ('grew', None), ('by', None), ('3', None), ('last', None), ('year', None), ('far', None), ('below', None), ('the', None)]\n",
      "Trigram tagger example: [('Chinas', None), ('economy', None), ('grew', None), ('by', None), ('3', None), ('last', None), ('year', None), ('far', None), ('below', None), ('the', None)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger \n",
    "ut = UnigramTagger(train_data) \n",
    "bt = BigramTagger(train_data) \n",
    "tt = TrigramTagger(train_data) \n",
    "unigram_tagger_accuracy = ut.evaluate(test_data) \n",
    "bigram_tagger_accuracy = bt.evaluate(test_data) \n",
    "trigram_tagger_accuracy = tt.evaluate(test_data) \n",
    "unigram_tagger_example = ut.tag(tokens) \n",
    "bigram_tagger_example = bt.tag(tokens) \n",
    "trigram_tagger_example = tt.tag(tokens) \n",
    "print(f'Unigram tagger accuracy: {unigram_tagger_accuracy:.2f}') \n",
    "print(f'Bigram tagger accuracy: {bigram_tagger_accuracy:.2f}') \n",
    "print(f'Trigram tagger accuracy: {trigram_tagger_accuracy:.2f}') \n",
    "print(f'Unigram tagger example: {unigram_tagger_example[:10]}') \n",
    "print(f'Bigram tagger example: {bigram_tagger_example[:10]}') \n",
    "print(f'Trigram tagger example: {trigram_tagger_example[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a57a9f-793d-4026-8276-8b117c456bad",
   "metadata": {},
   "source": [
    "# 📌 Combined POS Tagger with Backoff – NLTK\n",
    "\n",
    "In Part-of-Speech (POS) tagging, taggers like Unigram, Bigram, and Trigram use different levels of context to assign grammatical tags to words.\n",
    "\n",
    "A **combined tagger with backoff** improves tagging accuracy by falling back from complex models to simpler ones when a word cannot be tagged.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 What is Backoff?\n",
    "\n",
    "Backoff allows a tagger to use a simpler tagger if it cannot assign a tag:\n",
    "\n",
    "- **TrigramTagger**: Uses previous 2 tags for context.\n",
    "- If it fails, **back off** to → **BigramTagger**\n",
    "- If that fails, back off to → **UnigramTagger**\n",
    "- If all fail, fall back to → **DefaultTagger**, which assigns `'NN'` (noun) to everything.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0abab582-63c1-480f-8c9d-00003b9235db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6b5c9-c5c2-4d62-af0c-e49226890f85",
   "metadata": {},
   "source": [
    "### Classifier-Based POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b07e8-f26f-4183-bc89-8e06cbfc1521",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Unlike rule-based or n-gram taggers (like unigram/bigram), \n",
    "classifier-based taggers treat POS tagging as a supervised machine learning problem.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9898c697-e85e-40b3-8549-4de37c0af2ec",
   "metadata": {},
   "source": [
    "#### Naive Bayes POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d5024f0-f76e-4784-8345-0ae426755fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b5b0e053-a1d5-41d7-94d7-63e49f9544c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbt = ClassifierBasedPOSTagger(train=train_data, \n",
    "                               classifier_builder=NaiveBayesClassifier.train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3dc29-2b55-4898-97e5-0284f0fa38a7",
   "metadata": {},
   "source": [
    "train=train_data: Uses the training data (list of tagged sentences from Treebank).\n",
    "\n",
    "classifier_builder=...: Tells the tagger to use the Naive Bayes classifier.\n",
    "\n",
    "nbt: Is now a trained POS tagger using Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4cd11564-9727-4bcb-9ef7-a9a2d1b92ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadin\\AppData\\Local\\Temp\\ipykernel_9764\\274256967.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  nb_tagger_accuracy = nbt.evaluate(test_data)\n"
     ]
    }
   ],
   "source": [
    "nb_tagger_accuracy = nbt.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa340b33-c3ab-4ce9-b43a-82a7784efdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes tagger accuracy: 0.93\n",
      "Naive Bayes tagger example: [('Chinas', 'NNP'), ('economy', 'NN'), ('grew', 'VBD'), ('by', 'IN'), ('3', 'CD'), ('last', 'JJ'), ('year', 'NN'), ('far', 'RB'), ('below', 'IN'), ('the', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "nb_tagger_example = nbt.tag(tokens)\n",
    "\n",
    "print(f'Naive Bayes tagger accuracy: {nb_tagger_accuracy:.2f}')\n",
    "print(f'Naive Bayes tagger example: {nb_tagger_example[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9542a3d6-91dd-4b97-8c39-3c22f0db93cb",
   "metadata": {},
   "source": [
    "#### Maximum Entropy POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c091c68-12b0-4bbc-9854-b7cbc955d1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -3.82864        0.007\n",
      "             2          -0.76176        0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadin\\anaconda3\\Lib\\site-packages\\nltk\\classify\\maxent.py:1380: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2**nf_delta\n",
      "C:\\Users\\sadin\\anaconda3\\Lib\\site-packages\\nltk\\classify\\maxent.py:1382: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "C:\\Users\\sadin\\anaconda3\\Lib\\site-packages\\nltk\\classify\\maxent.py:1383: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Final               nan        0.984\n"
     ]
    }
   ],
   "source": [
    "met = ClassifierBasedPOSTagger(\n",
    "    train=train_data, \n",
    "    classifier_builder=lambda train_feats: MaxentClassifier.train(train_feats, max_iter=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0815cb2d-0060-45ac-8818-c278088476a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadin\\AppData\\Local\\Temp\\ipykernel_9764\\2511682021.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  maxent_tagger_accuracy = met.evaluate(test_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxent tagger accuracy: 0.93\n",
      "Maxent tagger example: [('Chinas', 'NNP'), ('economy', 'NN'), ('grew', 'VBD'), ('by', 'IN'), ('3', 'CD'), ('last', 'JJ'), ('year', 'NN'), ('far', 'RB'), ('below', 'IN'), ('the', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "maxent_tagger_accuracy = met.evaluate(test_data)\n",
    "maxent_tagger_example = met.tag(tokens)\n",
    "\n",
    "print(f'Maxent tagger accuracy: {maxent_tagger_accuracy:.2f}')\n",
    "print(f'Maxent tagger example: {maxent_tagger_example[:10]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8adde4-ddb4-43c2-ade5-6d554e020a76",
   "metadata": {},
   "source": [
    "# 📖 What is NER?\n",
    "NER identifies proper names in text and classifies them into categories such as:\n",
    "\n",
    "Person\n",
    "\n",
    "Organization\n",
    "\n",
    "Location\n",
    "\n",
    "Date\n",
    "\n",
    "Time\n",
    "\n",
    "Money, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "29c92194-0be7-4447-948d-48169eb9fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk #Performs named entity recognition on POS-tagged text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bbc30d2c-f488-4d04-a060-466fc9b5423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Barack Obama was born in Hawaii.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82764a62-b7ca-4c24-9720-a083991f9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = nltk.pos_tag(word_tokenize(text)) # Ensure correct input for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75358a94-f344-4884-b51e-fbfc8cc2b26e",
   "metadata": {},
   "source": [
    "\n",
    "##### text = \"Barack Obama was born in Hawaii.\"\n",
    "##### word_tokenize(text) → ['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii', '.']\n",
    "##### pos_tag(...) → [('Barack', 'NNP'), ('Obama', 'NNP'), ('was', 'VBD'), ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2f3c5b8f-13e2-48a0-aa6e-d2b2aa337731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'),\n",
       " ('cat', 'NN'),\n",
       " (',', ','),\n",
       " ('chootipoos', 'NN'),\n",
       " (',', ','),\n",
       " ('likes', 'VBZ'),\n",
       " ('dry', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('food', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('milk', 'NN')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11e8c7-dbe8-44a3-9ddf-06b6f5c0318d",
   "metadata": {},
   "source": [
    "## 🧠 Understanding `nltk.pos_tag()`\n",
    "\n",
    "| Aspect               | Details                                                                          |\n",
    "| -------------------- | -------------------------------------------------------------------------------- |\n",
    "| 🔍 **What is it?**       | A function in NLTK for **Part-of-Speech tagging**                                |\n",
    "| ⚙️ **How does it work?** | Uses a **Hidden Markov Model (HMM)**-based **pre-trained model**                 |\n",
    "| 📚 **Trained on what?**  | **Penn Treebank** corpus – a large collection of manually tagged text            |\n",
    "| 🛠 **Type of model?**    | **Statistical tagger** – it learns probabilities of tags based on word sequences |\n",
    "| 🧠 **Is it rule-based?** | ❌ No, it's not rule-based – it's statistical and **context-aware**               |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "- `nltk.pos_tag()` is a **pre-trained statistical POS tagger**.\n",
    "- It assigns part-of-speech tags to each token in a sentence.\n",
    "- It’s context-aware and trained on a large annotated corpus (Penn Treebank).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef76ac35-4c4e-4def-bfe2-0464ff1f07fd",
   "metadata": {},
   "source": [
    "#####\n",
    "\n",
    "If you're working with something like biomedical, legal, or social media text, the default Penn Treebank-based tagger (nltk.pos_tag) might not perform well because:\n",
    "\n",
    "The vocabulary is different\n",
    "\n",
    "Sentence structures may vary\n",
    "\n",
    "Slang or domain-specific words may be misclassified\n",
    "\n",
    "So, you’ll need to create your own  POS-tagged dataset (there are methods to create your oen POS-tagged datasets, or find a POS-tagged dataset in your target domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "72f9e4b5-8e4f-4306-af4b-7a5734bcd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### tagged_tokens = nltk.pos_tag(word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a4b0e8b5-9a70-4f6f-8ca3-41d20be6cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ne_chunk() is an NLTK function that performs Named Entity Recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5b58f0f5-84b0-4ced-be4a-67fb26b09f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## after doing pos tagging, we do chuncking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "95c8f366-67ae-4ab1-8b7a-67c2bcc7e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ner_tree = ne_chunk(tagged_tokens)\n",
    "\n",
    "# Step 3: Extract named entities\n",
    "named_entities = []\n",
    "for subtree in ner_tree:\n",
    "    if hasattr(subtree, 'label'):  # Check if it's a named entity\n",
    "        entity_name = ' '.join([child[0] for child in subtree])  # Get full name\n",
    "        entity_type = subtree.label()  # Get type (e.g., PERSON, GPE)\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "\n",
    "# Step 4: Print first 10 named entities\n",
    "print(named_entities[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4d71c5f4-32f5-401d-a146-8d4f95580969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barack', 'PERSON'), ('Obama', 'PERSON'), ('Hawaii', 'GPE'), ('White House', 'FACILITY')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "text = \"Barack Obama was born in Hawaii and worked at the White House.\"\n",
    "\n",
    "# Step 1: Tokenize and POS tag the text\n",
    "tokens = word_tokenize(text)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Step 2: Perform Named Entity Recognition\n",
    "ner_tree = ne_chunk(tagged_tokens)\n",
    "\n",
    "# Step 3: Extract named entities\n",
    "named_entities = []\n",
    "for subtree in ner_tree:\n",
    "    if hasattr(subtree, 'label'):  # Check if it's a named entity\n",
    "        entity_name = ' '.join([child[0] for child in subtree])  # Get full name\n",
    "        entity_type = subtree.label()  # Get type (e.g., PERSON, GPE)\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "\n",
    "# Step 4: Print first 10 named entities\n",
    "print(named_entities[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f4f1f-3f5b-481b-b8de-59882629f43d",
   "metadata": {},
   "source": [
    "### 🏗️ What is ne_chunk() based on?\n",
    "nltk.ne_chunk() uses the MaxEnt (Maximum Entropy) classifier, trained on the ACE and CONLL 2000/2003 corpora (these are datasets where humans manually labeled text with named entities).\n",
    "\n",
    "This model learns patterns like:\n",
    "\n",
    "If a person's name appears, it's often tagged as NNP (proper noun).\n",
    "\n",
    "It learns from context: If \"Barack Obama\" is followed by \"was president\", it's likely a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14798b79-9629-4626-9a4e-50095b02503a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
