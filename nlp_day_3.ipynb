{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP47CVsLNgyZ8Fw5GbbRsEe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yohan2001colombo/NLP/blob/main/nlp_day_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyphen\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v19nZNMaIHvu",
        "outputId": "a3d848d3-cb20-4585-f616-37ba4f27b67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyphen\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen\n",
            "Successfully installed pyphen-0.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CekGIAp8dGS3"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import epitran\n",
        "import panphon\n",
        "import matplotlib\n",
        "import pyphen\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Phonetic Transcription Using Pronouncing"
      ],
      "metadata": {
        "id": "wpA-goHyhdaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pronouncing\n",
        "\n",
        "words = [\"phonetics\",\"phonology\",\"morphology\",\"analysis\",\"transcription\"]\n",
        "transcriptions = {}\n",
        "\n",
        "for word in words:\n",
        "    transcription = pronouncing.phones_for_word(word)\n",
        "    transcriptions[word] = transcription[0] if transcription else \"No transcrption found\"\n",
        "print(transcriptions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvLviEUuftS-",
        "outputId": "209348e8-04ee-4a50-b699-b394e775ed42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'phonetics': 'F AH0 N EH1 T IH0 K S', 'phonology': 'F AH0 N AA1 L AH0 JH IY2', 'morphology': 'M AO0 R F AA1 L AH0 JH IY0', 'analysis': 'AH0 N AE1 L AH0 S AH0 S', 'transcription': 'T R AE2 N S K R IH1 P SH AH0 N'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Phonological Feature Extraction with Panphon"
      ],
      "metadata": {
        "id": "NLmpDSSeiXWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import panphon\n",
        "\n",
        "# define ARPAbet to IPA mapping (extend as needed)\n",
        "arpabet_to_ipa = arpabet_to_ipa = {\n",
        "    'P': 'p', 'B': 'b', 'K': 'k', 'D': 'd', 'G': 'g',\n",
        "    'T': 't', 'DH': 'ð', 'N': 'n', 'M': 'm', 'NG': 'ŋ',\n",
        "    'S': 's', 'Z': 'z', 'SH': 'ʃ', 'ZH': 'ʒ', 'CH': 'ʧ',\n",
        "    'JH': 'ʤ', 'Y': 'j', 'W': 'w', 'R': 'ɹ', 'L': 'l',\n",
        "    'Y': 'j', 'IY': 'iː', 'IH': 'ɪ', 'EH': 'ɛ', 'AE': 'æ',\n",
        "    'AA': 'ɑː', 'AO': 'ɔː', 'UH': 'ʊ', 'UW': 'uː', 'ER': 'ɜː',\n",
        "    'AH': 'ə', 'AW': 'aʊ', 'OY': 'ɔɪ', 'OW': 'oʊ', 'OY': 'ɔɪ',\n",
        "    'AY': 'aɪ', 'EH0': 'ə', 'IH0': 'ɪ', 'AH0': 'ə', 'UH0': 'ʊ',\n",
        "    'AW0': 'aʊ', 'OY0': 'ɔɪ', 'OW0': 'oʊ', 'AY0': 'aɪ'\n",
        "}\n",
        "# This expanded dictionary includes a broader range of ARPAbet symbols and their IPA equivalents. You can further extend this mapping based on your specific requirements.\n",
        "\n",
        "#Functional to convert ARPAbet to IPA\n",
        "def arpabet_to_ipa_converter(arpabet):\n",
        "    return ''.join([arpabet_to_ipa.get(ph.strip('0123456789'),'') for ph in arpabet.split()]) # The function strips these markers, potentially losing stress information.\n",
        "\n",
        "# Example transcription dictionary\n",
        "transcriptions = {'phonetics': 'F AH0 N EH1 T IH0 K S', 'phonology': 'F AH0 N AA1 L AH0 JH IY2', 'morphology': 'M AO0 R F AA1 L AH0 JH IY0', 'analysis': 'AH0 N AE1 L AH0 S AH0 S', 'transcription': 'T R AE2 N S K R IH1 P SH AH0 N'}\n",
        "\n",
        "ft = panphon.FeatureTable()\n",
        "\n",
        "# Generate phonological features for each word\n",
        "features = {\n",
        "    word : ft.word_fts(arpabet_to_ipa_converter(trans)) for word,trans in transcriptions.items()\n",
        "    if trans != \"No transcription found\"\n",
        "\n",
        "}\n",
        "# After converting ARPAbet transcriptions to IPA, the code uses PanPhon's FeatureTable class to analyze the phonological features of each word. The word_fts method returns a list of Segment objects, each representing a phoneme with its articulatory features\n",
        "\n",
        "# print the features\n",
        "print(features)\n",
        "\n",
        "# Each Segment object contains a list of features such as syl (syllabic), son (sonorant), cont (continuant), and so on, with values indicating their presence (+), absence (-), or neutrality (0).\n",
        "\n",
        "\n",
        "# These features can be utilized for various linguistic analyses, including phoneme classification, language comparison, and phonological pattern recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9IL_X0ueJjb",
        "outputId": "60a683fa-d45e-47ba-d15c-44b26fca16c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'phonetics': [<Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, +back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>, <Segment [-syl, +son, +cons, -cont, -delrel, -lat, +nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, -back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, -voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, +hi, -lo, -back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, -voi, -sg, -cg, -ant, -cor, 0distr, -lab, +hi, -lo, +back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, +cont, -delrel, -lat, -nas, +strid, -voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>], 'phonology': [<Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, +back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>, <Segment [-syl, +son, +cons, -cont, -delrel, -lat, +nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, 0delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, +lo, +back, -round, -velaric, +tense, +long, 0hitone, 0hireg]>, <Segment [-syl, +son, +cons, +cont, -delrel, +lat, -nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, +back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, +hi, -lo, -back, -round, -velaric, +tense, +long, 0hitone, 0hireg]>], 'morphology': [<Segment [-syl, +son, +cons, -cont, -delrel, -lat, +nas, -strid, +voi, -sg, -cg, +ant, -cor, 0distr, +lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, +back, +round, -velaric, -tense, +long, 0hitone, 0hireg]>, <Segment [-syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, +hi, -lo, -back, +round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, 0delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, +lo, +back, -round, -velaric, +tense, +long, 0hitone, 0hireg]>, <Segment [-syl, +son, +cons, +cont, -delrel, +lat, -nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, +back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, +hi, -lo, -back, -round, -velaric, +tense, +long, 0hitone, 0hireg]>], 'analysis': [<Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, +back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>, <Segment [-syl, +son, +cons, -cont, -delrel, -lat, +nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, +lo, -back, -round, -velaric, +tense, -long, 0hitone, 0hireg]>, <Segment [-syl, +son, +cons, +cont, -delrel, +lat, -nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, +back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, +cont, -delrel, -lat, -nas, +strid, -voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, +back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, +cont, -delrel, -lat, -nas, +strid, -voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>], 'transcription': [<Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, -voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [-syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, +hi, -lo, -back, +round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, +lo, -back, -round, -velaric, +tense, -long, 0hitone, 0hireg]>, <Segment [-syl, +son, +cons, -cont, -delrel, -lat, +nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, +cont, -delrel, -lat, -nas, +strid, -voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, -voi, -sg, -cg, -ant, -cor, 0distr, -lab, +hi, -lo, +back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [-syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, +hi, -lo, -back, +round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, +hi, -lo, -back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, -voi, -sg, -cg, +ant, -cor, 0distr, +lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, +cont, -delrel, -lat, -nas, +strid, -voi, -sg, -cg, -ant, +cor, +distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, +back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>, <Segment [-syl, +son, +cons, -cont, -delrel, -lat, +nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phonome Frequency Analysis"
      ],
      "metadata": {
        "id": "e2cgCBJfnseU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "#split all transcriptions into phonemes(by space),then\n",
        "all_phonemes = \"\".join(transcriptions.values()).split()\n",
        "phoneme_counts = Counter(all_phonemes)\n",
        "\n",
        "print(phoneme_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eM1_4t4nLpX",
        "outputId": "2cbbbff6-5430-4a6f-b52e-27e0c3f3275f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'AH0': 7, 'N': 5, 'L': 3, 'R': 3, 'F': 2, 'K': 2, 'AA1': 2, 'JH': 2, 'S': 2, 'EH1': 1, 'T': 1, 'IH0': 1, 'SF': 1, 'IY2M': 1, 'AO0': 1, 'IY0AH0': 1, 'AE1': 1, 'ST': 1, 'AE2': 1, 'IH1': 1, 'P': 1, 'SH': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join(transcriptions.values()).split();"
      ],
      "metadata": {
        "id": "nm9IpeBrdpS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stress Pattern Analysis with CMU Dictionary"
      ],
      "metadata": {
        "id": "4l6Q40zinuvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('cmudict')"
      ],
      "metadata": {
        "id": "gKMAsXSupq2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9db1c9-ba4a-45e9-9ada-9f1df0abcbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import cmudict\n",
        "\n",
        "# Initialize the CMU Pronouncing Dictionary\n",
        "d = cmudict.dict()\n",
        "\n",
        "# Make sure 'words' is defined\n",
        "words = ['phonetics','phonology','morphology','analysis','transcription']\n",
        "\n",
        "stress_patterns = {}\n",
        "\n",
        "for word in words:\n",
        "  if word in d:\n",
        "    trans = d[word][0]\n",
        "    pattern = ''.join(['1' if '1' in ph else '0' for ph in trans])\n",
        "    stress_patterns[word] = pattern\n",
        "  else:\n",
        "    stress_patterns[word] = \"No found\"\n",
        "\n",
        "print(stress_patterns)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AymmNcU9n0ka",
        "outputId": "b9babb84-8995-4194-b9f5-51258e620479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'phonetics': '00010000', 'phonology': '00010000', 'morphology': '000010000', 'analysis': '00100000', 'transcription': '000000010000'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d[\"phonetics\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN1xcB05bHCz",
        "outputId": "673f1ee2-9724-4ef1-edf1-e852f40a67e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['F', 'AH0', 'N', 'EH1', 'T', 'IH0', 'K', 'S']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Vowel and Consonants Analysis"
      ],
      "metadata": {
        "id": "O17e3SedrE-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Define ARPAbet vowel phonemes\n",
        "arpabwt_vowels = {\"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\", \"EH\", \"ER\", \"EY\", \"IH\",\"IY\", \"OW\", \"OY\", \"UH\", \"UW\"}\n",
        "\n",
        "#combine all the transcriptions and split into phonemes\n",
        "phonemes = \" \".join(transcriptions.values()).split()\n",
        "\n",
        "# Seperate vowels and consonants (strip stress digits)\n",
        "vowels = [ph for ph in phonemes if ph.strip('0123456789') in arpabwt_vowels]\n",
        "consonants = [ph for ph in phonemes if ph.strip('0123456789') not in arpabwt_vowels]\n",
        "\n",
        "# Count\n",
        "vowel_count = Counter(vowels)\n",
        "consonant_count = Counter(consonants)\n",
        "\n",
        "print(\"Vowels:\",vowel_count)\n",
        "print(\"Consonants:\",consonant_count)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hNy9F_EpCQR",
        "outputId": "aa02a468-005a-45a7-c7b8-686a90f77343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vowels: Counter({'AH0': 8, 'AA1': 2, 'EH1': 1, 'IH0': 1, 'IY2': 1, 'AO0': 1, 'IY0': 1, 'AE1': 1, 'AE2': 1, 'IH1': 1})\n",
            "Consonants: Counter({'N': 5, 'S': 4, 'F': 3, 'L': 3, 'R': 3, 'T': 2, 'K': 2, 'JH': 2, 'M': 1, 'P': 1, 'SH': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaVqg2OAhQVI",
        "outputId": "4ea21552-7ebe-47be-f762-ddfae3d8372e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AH0',\n",
              " 'EH1',\n",
              " 'IH0',\n",
              " 'AH0',\n",
              " 'AA1',\n",
              " 'AH0',\n",
              " 'IY2',\n",
              " 'AO0',\n",
              " 'AA1',\n",
              " 'AH0',\n",
              " 'IY0',\n",
              " 'AH0',\n",
              " 'AE1',\n",
              " 'AH0',\n",
              " 'AH0',\n",
              " 'AE2',\n",
              " 'IH1',\n",
              " 'AH0']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the punkt tokenizer data\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# sample text\n",
        "text = \"Hello! This is a quick test\"\n",
        "\n",
        "# Tokenize\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws_zzGrEtKDm",
        "outputId": "2cf027e5-f945-4c07-c104-c95b0620cb9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'This', 'is', 'a', 'quick', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necesary data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZDep7PJwQtl",
        "outputId": "9f401820-3140-4714-98af-9d72cbe46ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Tokenize text into words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "\n",
        "# Remove Engilsh stopwords\n",
        "filtered = [w for w in words if w not in stopwords.words('english')]\n",
        "\n",
        "# Initialie stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply stemming and lemmatiation\n",
        "stemmed = [stemmer.stem(word) for word in words]\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"filtered words\", filtered)\n",
        "print(\"Lemmatized:\",lemmatized)\n",
        "print(\"Stemmed_words:\",stemmed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEPhyCaEunaz",
        "outputId": "2d28db4a-c359-4ee2-f416-e13bfaa1749b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filtered words ['The', 'striped', 'bats', 'hanging', 'feet', 'best']\n",
            "Lemmatized: ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best']\n",
            "Stemmed_words: ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 Morphological Complexity and Affis Analysis"
      ],
      "metadata": {
        "id": "oLZmiE46sF46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "lengths = [len(w) for w in filtered]\n",
        "average_length = sum(lengths)/len(lengths) if lengths else 0\n",
        "print(\"Average word length:\",average_length)\n",
        "\n",
        "prefixes = Counter([w[:3] for w in filtered if len(w)<3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDE61rkUxQPo",
        "outputId": "7186ac51-0203-40a9-8994-9e35ec55a21e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average word length: 4.833333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 Syllable and Compound Word Analysis"
      ],
      "metadata": {
        "id": "kyaq461Ryd_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyphen\n",
        "\n",
        "dic = pyphen.Pyphen(lang='en')\n",
        "syllables = [len(dic.inserted(w).split('-')) for w in filtered if w.strip() != '']\n",
        "\n",
        "avg_syllabes = sum(syllables)/len(syllables) if syllables else 0\n",
        "print(\"Avg syllables per word:\",avg_syllabes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0nLlmsOynch",
        "outputId": "91d6f440-29cd-49da-b07d-7647274a4bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg syllables per word: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lpt11AiMydg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}