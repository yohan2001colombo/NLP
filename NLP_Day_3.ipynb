{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZELawuwdn9Mbq/AuSV64I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yohan2001colombo/NLP/blob/main/NLP_Day_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CekGIAp8dGS3"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import epitran\n",
        "import panphon\n",
        "import matplotlib\n",
        "import pyphen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Phonetic Transcription Using Pronouncing"
      ],
      "metadata": {
        "id": "wpA-goHyhdaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pronouncing\n",
        "\n",
        "words = [\"phonetics\",\"phonology\",\"morphology\",\"analysis\",\"transcription\"]\n",
        "transcriptions = {}\n",
        "\n",
        "for word in words:\n",
        "    transcription = pronouncing.phones_for_word(word)\n",
        "    transcriptions[word] = transcription[0] if transcription else \"No transcrption found\"\n",
        "print(transcriptions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvLviEUuftS-",
        "outputId": "42c77f81-4a91-4d78-daea-6bbcf6c7f4cc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'phonetics': 'F AH0 N EH1 T IH0 K S', 'phonology': 'F AH0 N AA1 L AH0 JH IY2', 'morphology': 'M AO0 R F AA1 L AH0 JH IY0', 'analysis': 'AH0 N AE1 L AH0 S AH0 S', 'transcription': 'T R AE2 N S K R IH1 P SH AH0 N'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Phonological Feature Extraction with Panphon"
      ],
      "metadata": {
        "id": "NLmpDSSeiXWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import panphon\n",
        "\n",
        "# define ARPAbet to IPA mapping (extend as needed)\n",
        "arpabet_to_ipa = {'P':\"p\",\"T\":\"t\",\"K\":\"k\",\"B\":\"b\",\"D\":\"d\",\"G\":\"g\",\n",
        "                  # Extend with more mappingd as needed\n",
        "                  }\n",
        "\n",
        "#Functional to convert ARPAbet to IPA\n",
        "def arpabet_to_ipa_converter(arpabet):\n",
        "    return ''.join([arpabet_to_ipa.get(ph.strip('0123456789'),'') for ph in arpabet.split()])\n",
        "\n",
        "# Example transcription dictionary\n",
        "transcriptions = {'phonetics': 'F AH0 N EH1 T IH0 K S', 'phonology': 'F AH0 N AA1 L AH0 JH IY2', 'morphology': 'M AO0 R F AA1 L AH0 JH IY0', 'analysis': 'AH0 N AE1 L AH0 S AH0 S', 'transcription': 'T R AE2 N S K R IH1 P SH AH0 N'}\n",
        "\n",
        "ft = panphon.FeatureTable()\n",
        "\n",
        "# Generate phonological features for each word\n",
        "features = {\n",
        "    word : ft.word_fts(arpabet_to_ipa_converter(trans)) for word,trans in transcriptions.items()\n",
        "    if trans != \"No transcription found\"\n",
        "}\n",
        "# print the features\n",
        "print(features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9IL_X0ueJjb",
        "outputId": "2d47e4b5-c937-41a7-e0d9-3e77ecbe5661"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'phonetics': [<Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, -voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, -voi, -sg, -cg, -ant, -cor, 0distr, -lab, +hi, -lo, +back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>], 'phonology': [], 'morphology': [], 'analysis': [], 'transcription': [<Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, -voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, -voi, -sg, -cg, -ant, -cor, 0distr, -lab, +hi, -lo, +back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>, <Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, -voi, -sg, -cg, +ant, -cor, 0distr, +lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phonome Frequency Analysis"
      ],
      "metadata": {
        "id": "e2cgCBJfnseU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "#split all transcriptions into phonemes(by space),then\n",
        "all_phonemes = \"\".join(transcriptions.values()).split()\n",
        "phoneme_counts = Counter(all_phonemes)\n",
        "\n",
        "print(phoneme_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eM1_4t4nLpX",
        "outputId": "c66ec90a-5b38-4fc7-9d8f-e8cfbbd27c00"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'AH0': 7, 'N': 5, 'L': 3, 'R': 3, 'F': 2, 'K': 2, 'AA1': 2, 'JH': 2, 'S': 2, 'EH1': 1, 'T': 1, 'IH0': 1, 'SF': 1, 'IY2M': 1, 'AO0': 1, 'IY0AH0': 1, 'AE1': 1, 'ST': 1, 'AE2': 1, 'IH1': 1, 'P': 1, 'SH': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stress Pattern Analysis with CMU Dictionary"
      ],
      "metadata": {
        "id": "4l6Q40zinuvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('cmudict')"
      ],
      "metadata": {
        "id": "gKMAsXSupq2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import cmudict\n",
        "\n",
        "# Initialize the CMU Pronouncing Dictionary\n",
        "d = cmudict.dict()\n",
        "\n",
        "# Make sure 'words' is defined\n",
        "words = ['phonetics','phonology','morphology','analysis','transcription']\n",
        "\n",
        "stress_patterns = {}\n",
        "\n",
        "for word in words:\n",
        "  if word in d:\n",
        "    trans = d[word][0]\n",
        "    pattern = ''.join(['1' if '1' in ph else '0' for ph in trans])\n",
        "    stress_patterns[word] = pattern\n",
        "  else:\n",
        "    stress_patterns[word] = \"No found\"\n",
        "\n",
        "print(stress_patterns)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AymmNcU9n0ka",
        "outputId": "83823de1-53ae-4ff1-e70f-9bc7a62f7276"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'phonetics': '00010000', 'phonology': '00010000', 'morphology': '000010000', 'analysis': '00100000', 'transcription': '000000010000'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Vowel and Constant Analysis"
      ],
      "metadata": {
        "id": "O17e3SedrE-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Define ARPAbet vowel phonemes\n",
        "arpabwt_vowels = {\"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\", \"EH\", \"ER\", \"EY\", \"IH\",\"IY\", \"OW\", \"OY\", \"UH\", \"UW\"}\n",
        "\n",
        "#combine all the transcriptions and split into phonemes\n",
        "phonemes = \" \".join(transcriptions.values()).split()\n",
        "\n",
        "# Seperate vowels and consonants (strip stress digits)\n",
        "vowels = [ph for ph in phonemes if ph.strip('0123456789') in arpabwt_vowels]\n",
        "consonants = [ph for ph in phonemes if ph.strip('0123456789') not in arpabwt_vowels]\n",
        "\n",
        "# Count\n",
        "vowel_count = Counter(vowels)\n",
        "consonant_count = Counter(consonants)\n",
        "\n",
        "print(\"Vowels:\",vowel_count)\n",
        "print(\"Consonants:\",consonant_count)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hNy9F_EpCQR",
        "outputId": "9e01e373-3a7e-47c0-9554-aa15a56d22ab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vowels: Counter({'AH0': 8, 'AA1': 2, 'EH1': 1, 'IH0': 1, 'IY2': 1, 'AO0': 1, 'IY0': 1, 'AE1': 1, 'AE2': 1, 'IH1': 1})\n",
            "Consonants: Counter({'N': 5, 'S': 4, 'F': 3, 'L': 3, 'R': 3, 'T': 2, 'K': 2, 'JH': 2, 'M': 1, 'P': 1, 'SH': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the punkt tokenizer data\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# sample text\n",
        "text = \"Hello! This is a quick test\"\n",
        "\n",
        "# Tokenize\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws_zzGrEtKDm",
        "outputId": "49de10cb-4bbb-4f2e-96bf-7509f7d598f2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'This', 'is', 'a', 'quick', 'test']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necesary data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZDep7PJwQtl",
        "outputId": "3f0d0f04-8e2d-4a6b-8765-44a10732bc45"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Tokenize text into words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "\n",
        "# Remove Engilsh stopwords\n",
        "filtered = [w for w in words if w not in stopwords.words('english')]\n",
        "\n",
        "# Initialie stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply stemming and lemmatiation\n",
        "stemmed = [stemmer.stem(word) for word in words]\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"filtered words\", filtered)\n",
        "print(\"Lemmatized:\",lemmatized)\n",
        "print(\"Stemmed_words:\",stemmed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEPhyCaEunaz",
        "outputId": "d78b58eb-211f-450f-bc5d-41e73daa8b82"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filtered words ['The', 'striped', 'bats', 'hanging', 'feet', 'best']\n",
            "Lemmatized: ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best']\n",
            "Stemmed_words: ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 Morphological Complexity and Affis Analysis"
      ],
      "metadata": {
        "id": "oLZmiE46sF46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "lengths = [len(w) for w in filtered]\n",
        "average_length = sum(lengths)/len(lengths) if lengths else 0\n",
        "print(\"Average word length:\",average_length)\n",
        "\n",
        "prefixes = Counter([w[:3] for w in filtered if len(w)<3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDE61rkUxQPo",
        "outputId": "06871e77-f457-4c1b-e5fe-246ca0b62181"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average word length: 4.833333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 Syllable and Compound Word Analysis"
      ],
      "metadata": {
        "id": "kyaq461Ryd_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyphen\n",
        "\n",
        "dic = pyphen.Pyphen(lang='en')\n",
        "syllables = [len(dic.inserted(w).split('-')) for w in filtered if w.strip() != '']\n",
        "\n",
        "avg_syllabes = sum(syllables)/len(syllables) if syllables else 0\n",
        "print(\"Avg syllables per word:\",avg_syllabes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0nLlmsOynch",
        "outputId": "c1fef15e-7d72-4301-c7fa-d9d247174b9e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg syllables per word: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lpt11AiMydg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}